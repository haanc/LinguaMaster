# Bug Fix: 翻译功能不使用用户配置的 LLM 提供商

## 基本信息

| 项目 | 内容 |
|------|------|
| **发现时间** | 2026-01-18 19:30 (UTC+8) |
| **解决时间** | 2026-01-18 21:20 (UTC+8) |
| **修复耗时** | 约 110 分钟 |
| **影响范围** | 字幕翻译功能 |
| **严重程度** | 高 |
| **修复版本** | v0.0.5 |

---

## Bug 描述

### 现象
用户在设置中配置了 Azure APIM 作为 LLM 提供商，测试连接成功。但点击翻译按钮后，翻译显示"完成"，却没有任何翻译结果显示在界面上。

数据库检查发现所有 89 个字幕段落的 `translation` 字段都为空。

### 复现步骤
1. 在设置页面配置自定义 LLM 提供商（如 Azure APIM）
2. 点击"测试连接"，确认连接成功
3. 导入一个视频并完成转录
4. 点击"翻译"按钮
5. 翻译显示完成，但字幕面板和右侧翻译区域都没有显示翻译内容

---

## 根本原因分析

问题出在翻译端点没有使用用户配置的 LLM，而是使用服务器环境变量中的默认配置。

### 架构分析

用户的 LLM 配置通过 `X-LLM-Config` HTTP 头传递给后端：

```
前端 -> [X-LLM-Config header] -> 后端 -> LLM API
```

其他 AI 端点（如 `/ai/lookup-word`、`/ai/test-connection`）正确使用了 `get_request_llm_provider` 依赖注入来获取用户配置的 LLM：

```python
# routes/ai.py - 正确实现
@router.post("/lookup-word")
def lookup_word(req: LookupRequest, llm_provider: LLMProvider = Depends(get_request_llm_provider)):
    return ai_service.lookup_word_with_provider(..., llm_provider)
```

但翻译端点 `/media/{id}/translate` 没有使用这个依赖：

```python
# routes/media.py - 问题代码
@router.post("/{media_id}/translate")
def translate_segments(media_id: UUID, req: TranslateSegmentsRequest, session: Session = Depends(get_session)):
    # 直接调用 translate_batch，没有传入 LLM provider
    translations = ai_service.translate_batch(texts_to_translate, req.target_language)
```

而 `ai_service.translate_batch()` 内部调用 `get_llm_provider()` 获取的是环境变量配置的默认 LLM（Ollama），不是用户配置的 Azure APIM。

### 前端问题

前端的 axios 拦截器只对 `/ai/` 路径注入 `X-LLM-Config` 头：

```typescript
// 问题代码
if (config.url?.includes('/ai/')) {
    config.headers['X-LLM-Config'] = llmConfigHeader;
}
```

翻译端点 `/media/{id}/translate` 不包含 `/ai/`，因此不会注入 LLM 配置头。

---

## 修复方案

### 修复 1: 扩展 ai_service.translate_batch() 支持外部 LLM Provider

**文件**: `backend/ai_service.py`

```python
def translate_batch(
    self,
    texts: List[str],
    target_language: str = "Chinese",
    llm_provider: Optional["LLMProvider"] = None,  # 新增参数
) -> Dict[int, str]:
    if not texts:
        return {}

    # Use provided provider or fall back to global config
    if llm_provider is None:
        from ai.providers import get_llm_provider
        llm_provider = get_llm_provider()

    # ... 其余代码使用 llm_provider
    llm = llm_provider.get_chat_model(temperature=0.3)
```

添加了可选的 `llm_provider` 参数，保持向后兼容。

### 修复 2: 翻译端点使用 LLM Provider 依赖注入

**文件**: `backend/routes/media.py`

```python
from ai.dependencies import get_request_llm_provider
from ai.providers.llm import LLMProvider

@router.post("/{media_id}/translate")
def translate_segments(
    media_id: UUID,
    req: TranslateSegmentsRequest,
    session: Session = Depends(get_session),
    llm_provider: LLMProvider = Depends(get_request_llm_provider),  # 新增
):
    """
    Translate specified segments with multi-layer caching.
    Accepts X-LLM-Config header for user-configured LLM.
    """
    # ...
    translations = ai_service.translate_batch(
        texts_to_translate,
        req.target_language,
        llm_provider  # 传入用户配置的 LLM
    )
```

### 修复 3: 前端拦截器支持翻译端点

**文件**: `src/services/api.ts`

```typescript
axios.interceptors.request.use((config) => {
    // Inject LLM config for AI-related endpoints and translate endpoint
    const needsLlmConfig = config.url?.includes('/ai/') || config.url?.includes('/translate');
    if (needsLlmConfig) {
        const llmConfigHeader = llmConfigStorage.getRequestHeader();
        if (llmConfigHeader) {
            config.headers['X-LLM-Config'] = llmConfigHeader;
        }
    }
    return config;
});
```

扩展了 URL 匹配规则，包含 `/translate` 路径。

---

## 技术要点

### 依赖注入模式

FastAPI 的 `Depends()` 允许在端点函数签名中声明依赖，自动从请求中提取数据并注入：

```python
def get_request_llm_provider(
    x_llm_config: Optional[str] = Header(None, alias="X-LLM-Config")
) -> LLMProvider:
    if not x_llm_config:
        return get_llm_provider()  # 回退到默认配置

    config_dict = json.loads(base64.b64decode(x_llm_config))
    return get_llm_provider_from_request(config_dict)
```

### 可选参数保持向后兼容

修改 `translate_batch()` 时使用可选参数 `llm_provider: Optional[...] = None`，确保：
1. 现有调用代码无需修改
2. 不传参数时自动使用默认 LLM
3. 传入参数时使用用户配置的 LLM

---

## 验证步骤

1. 运行 `npm run build:full` 生成新的安装包（v0.0.5）
2. 安装并启动应用
3. 在设置中配置自定义 LLM 提供商（如 Azure APIM）
4. 点击"测试连接"，确认成功
5. 导入视频并完成转录
6. 点击"翻译"按钮
7. 确认翻译结果正确显示在字幕面板和右侧翻译区域
8. 检查后端日志，确认使用的是用户配置的 LLM（如 "Azure APIM (gpt-4o)"）

---

## 相关文件

| 文件 | 修改类型 |
|------|----------|
| `backend/ai_service.py` | 修改 |
| `backend/routes/media.py` | 修改 |
| `src/services/api.ts` | 修改 |
| `package.json` | 修改（版本号更新至 0.0.5）|

---

## 后续优化建议

1. **统一 AI 端点路径**: 考虑将翻译端点迁移到 `/ai/translate`，保持 AI 功能端点的一致性
2. **配置验证**: 在翻译前检查 LLM 配置是否有效，提前给出错误提示
3. **批量翻译优化**: 对于大量字幕，考虑分批请求并显示进度
4. **错误处理**: 翻译失败时保留错误信息，方便用户排查
