# Bug Fix: 视频 Streaming 500 错误 & 翻译显示不完整

## 基本信息

| 项目 | 内容 |
|------|------|
| **发现时间** | 2026-01-19 11:30 (UTC+8) |
| **解决时间** | 2026-01-19 12:15 (UTC+8) |
| **修复耗时** | 约 45 分钟 |
| **影响范围** | 视频播放功能、双语字幕翻译功能 |
| **严重程度** | 高 |

---

## Bug 描述

### 现象

本次修复涉及两个相关问题：

**Bug 1: 视频 Streaming 500 错误**
- 用户在 twili 测试机上运行 LinguaMaster，可以下载音频
- 但点击视频准备 streaming 播放时，显示 500 Internal Server Error

**Bug 2: 翻译显示不完整**
- 用户触发全文翻译后，右侧字幕面板和中间播放器的翻译无法完全显示
- 1282 个字幕段落中只有 235 个显示翻译，1047 个没有翻译
- 翻译结果呈现"有些段落有，有些没有"的随机状态

### 复现步骤

**Bug 1:**
1. 在 twili 测试机启动 LinguaMaster 安装版
2. 添加一个 YouTube 视频 URL
3. 等待下载完成
4. 点击视频进入播放页面
5. 观察到 500 错误

**Bug 2:**
1. 打开一个已转录的视频
2. 选择目标语言（如中文）
3. 点击翻译按钮开启双语字幕
4. 观察到部分字幕有翻译，部分没有

---

## 根本原因分析

### Bug 1: media_service.py 代码缺陷

1. **缺少 subprocess 导入**
   - `fetch_stream_url` 方法使用了 `subprocess.run()` 但文件顶部未导入 `subprocess` 模块
   - 导致调用该方法时抛出 `NameError: name 'subprocess' is not defined`

2. **命令名拼写错误**
   - 代码中写的是 `"yt_dlp"`（下划线）
   - 但实际的命令行工具名是 `"yt-dlp"`（破折号）
   - 导致子进程调用失败

### Bug 2: translate_batch 未分批处理

- `translate_batch` 方法一次性将所有待翻译的 segments 发送给 LLM
- 当有 1047 个 segments 需要翻译时，输入内容超出 LLM 的 token 限制
- LLM 输出被截断，只返回了前 235 个翻译
- 解析逻辑只能获取到截断前的翻译结果

---

## 修复方案

### 修复 1: 添加 subprocess 导入

**文件**: `backend/media_service.py`

```python
# 修复前
import yt_dlp
import os
from typing import Optional, Callable

# 修复后
import yt_dlp
import os
import subprocess
from typing import Optional, Callable
```

添加缺失的 `subprocess` 模块导入，确保 `fetch_stream_url` 方法能正常调用子进程。

### 修复 2: 修正 yt-dlp 命令名

**文件**: `backend/media_service.py`

```python
# 修复前
cmd = [
    "yt_dlp",  # 错误：使用了下划线
    "-g",
    "-f", "best[ext=mp4]/best",
    url
]

# 修复后
cmd = [
    "yt-dlp",  # 正确：使用破折号
    "-g",
    "-f", "best[ext=mp4]/best",
    url
]
```

### 修复 3: 翻译分批处理

**文件**: `backend/ai_service.py`

```python
def translate_batch(
    self,
    texts: List[str],
    target_language: str = "Chinese",
    llm_provider: Optional["LLMProvider"] = None,
    batch_size: int = 20,  # 新增：每批处理 20 个 segments
) -> Dict[int, str]:
    """
    Translate multiple texts in batched API calls to avoid token limits.
    """
    if not texts:
        return {}

    all_translations: Dict[int, str] = {}
    total_batches = (len(texts) + batch_size - 1) // batch_size

    print(f"Translating {len(texts)} segments in {total_batches} batches")

    for batch_num in range(total_batches):
        start_idx = batch_num * batch_size
        end_idx = min(start_idx + batch_size, len(texts))
        batch_texts = texts[start_idx:end_idx]

        # 使用全局索引准备批次文本
        batch_text = "\n---\n".join(
            [f"[{start_idx + i}] {t}" for i, t in enumerate(batch_texts)]
        )

        try:
            llm = llm_provider.get_chat_model(temperature=0.3)
            response = llm.invoke([...])
            # 解析响应并添加到 all_translations
            ...
            print(f"  Batch {batch_num + 1}/{total_batches}: done")

        except Exception as e:
            print(f"Error in batch {batch_num + 1}: {e}")
            continue  # 继续处理下一批，不中断整个流程

    return all_translations
```

**关键改进：**
- 将 segments 分成每批 20 个处理
- 1047 个 segments 会分成 53 个批次
- 即使某个批次失败，其他批次仍会继续
- 添加进度日志便于调试

---

## 技术要点

### 1. Python 模块导入规范
- 标准库导入应放在文件顶部
- 使用模块前必须确保已导入
- 静态类型检查工具（如 mypy）可以提前发现此类问题

### 2. LLM Token 限制
- 大多数 LLM 有输入/输出 token 限制
- 批量处理时需要考虑分批策略
- 建议预估 token 数量并动态调整批次大小

### 3. 优雅降级
- 批量操作失败时不应该全部回滚
- 采用"尽可能多处理"策略
- 记录失败项以便后续重试

---

## 验证步骤

### Bug 1 验证:
1. 重启 LinguaMaster 应用
2. 打开已下载的视频
3. 确认视频可以正常播放，无 500 错误

### Bug 2 验证:
1. 打开一个已转录的视频
2. 关闭翻译显示
3. 重新开启翻译显示
4. 观察控制台日志显示分批翻译进度
5. 确认所有字幕段落都显示翻译

---

## 相关文件

| 文件 | 修改类型 |
|------|----------|
| `backend/media_service.py` | 修改 - 添加 subprocess 导入，修正 yt-dlp 命令名 |
| `backend/ai_service.py` | 修改 - 添加翻译分批处理逻辑 |

---

## 后续优化建议

1. **动态批次大小**: 根据 segment 平均长度动态计算 batch_size，避免单批 token 过多

2. **翻译进度显示**: 前端显示翻译进度条（已完成/总数），提升用户体验

3. **失败重试机制**: 记录翻译失败的 segment IDs，支持手动或自动重试

4. **预检查机制**: 在构建打包前增加静态分析步骤，检测未使用的导入或缺失的导入

5. **单元测试覆盖**: 为 `translate_batch` 添加测试用例，验证大批量输入场景

---

*此文档由 Claude Code 自动生成*
